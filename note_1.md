* token: 词汇表里的选项， 组成不同序列
* 自回归语言模型：基于链式法则,逐个单词生成
* 非自回归语言模型：基于链式法则,生成多个单词
* 温度较高随机性大：退火过程
* chatgpt使用3000亿个token
* n-gram无法利用长距离信息
* RNN/LSTM使n趋于无穷，利用整个上下文， Transformaer固定n， 神经网络生成句子的向量表示可用于微调
* 上下文学习训练一个模型完成多个任务

课程：理解大模型的行为->数据->模型->抽象
