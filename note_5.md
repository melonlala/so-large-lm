# 大模型的训练数据：原始文本
google索引100PB

1PB = 1024（$2^10$）TB，以此类推 GM MB KB B

GPT-2：WebText 40GB，OpenWebText38GB

C4:806GB

The Pile:825GB较小高质量数据集

